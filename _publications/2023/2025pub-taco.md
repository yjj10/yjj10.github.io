---
title:          "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration"
date:           2025-08-21 00:01:00 +0800
selected:       true
pub:            "EMNLP 2025 Main Conference (Accepted)"
pub_date:       "2025"
abstract: >-
  Multimodal in-context learning (ICL) has emerged as a key mechanism for harnessing the capabilities of large
  vision-language models (LVLMs). However, its effectiveness remains highly sensitive to the quality of input
  sequences. We propose "TACO", a lightweight transformer-based model guided by task mapping, which dynamically
  configures in-context sequences via task-aware attention. TACO enables a bidirectional synergy between
  sequence construction and task reasoning, consistently surpassing baselines on five LVLMs and nine benchmarks.
  These results highlight task mapping as a valuable perspective for improving multimodal ICL.
cover:          /assets/images/photos/taco.jpg
authors:
- "Yanshu Li"
- "Jianjiang Yang*"
- "Tian Yun"
- "Pinyuan Feng"
- "Jinfa Huang"
- "Ruixiang Tang"
links:
  Paper (arXiv): https://arxiv.org/abs/2505.17098
 
---
